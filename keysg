import os
import json
import argparse
import numpy as np
import imageio.v2 as imageio
from natsort import natsorted
from tqdm import tqdm

import open3d as o3d
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

from skimage.filters import gaussian
from skimage.segmentation import watershed
from skimage.feature import peak_local_max
from scipy import ndimage as ndi


# ---------------- IO helpers ----------------
def load_intrinsics_txt_4x4(path):
    K = np.loadtxt(path).astype(np.float32)
    fx, fy, cx, cy = float(K[0, 0]), float(K[1, 1]), float(K[0, 2]), float(K[1, 2])
    return fx, fy, cx, cy, K


def list_scannet_files(scene_dir):
    color_dir = os.path.join(scene_dir, "color")
    depth_dir = os.path.join(scene_dir, "depth")
    pose_dir  = os.path.join(scene_dir, "pose")

    color_paths = natsorted([os.path.join(color_dir, f) for f in os.listdir(color_dir) if f.endswith(".jpg")])
    depth_paths = natsorted([os.path.join(depth_dir, f) for f in os.listdir(depth_dir) if f.endswith(".png")])
    pose_paths  = natsorted([os.path.join(pose_dir,  f) for f in os.listdir(pose_dir)  if f.endswith(".txt")])

    if not (len(color_paths) == len(depth_paths) == len(pose_paths)):
        raise ValueError(f"count mismatch color={len(color_paths)} depth={len(depth_paths)} pose={len(pose_paths)}")
    return color_paths, depth_paths, pose_paths


def load_pose_4x4(path, pose_is_w2c=False):
    T = np.loadtxt(path).astype(np.float32)
    if T.shape != (4, 4):
        raise ValueError(f"pose not 4x4: {path} got {T.shape}")
    if pose_is_w2c:
        T = np.linalg.inv(T)
    return T


def rot_to_quat_wxyz(R):
    """Robust quaternion (w,x,y,z) from rotation matrix."""
    m = R
    t = np.trace(m)
    if t > 0.0:
        s = np.sqrt(t + 1.0) * 2.0
        w = 0.25 * s
        x = (m[2, 1] - m[1, 2]) / s
        y = (m[0, 2] - m[2, 0]) / s
        z = (m[1, 0] - m[0, 1]) / s
    else:
        if (m[0, 0] > m[1, 1]) and (m[0, 0] > m[2, 2]):
            s = np.sqrt(1.0 + m[0, 0] - m[1, 1] - m[2, 2]) * 2.0
            w = (m[2, 1] - m[1, 2]) / s
            x = 0.25 * s
            y = (m[0, 1] + m[1, 0]) / s
            z = (m[0, 2] + m[2, 0]) / s
        elif m[1, 1] > m[2, 2]:
            s = np.sqrt(1.0 + m[1, 1] - m[0, 0] - m[2, 2]) * 2.0
            w = (m[0, 2] - m[2, 0]) / s
            x = (m[0, 1] + m[1, 0]) / s
            y = 0.25 * s
            z = (m[1, 2] + m[2, 1]) / s
        else:
            s = np.sqrt(1.0 + m[2, 2] - m[0, 0] - m[1, 1]) * 2.0
            w = (m[1, 0] - m[0, 1]) / s
            x = (m[0, 2] + m[2, 0]) / s
            y = (m[1, 2] + m[2, 1]) / s
            z = 0.25 * s
    q = np.array([w, x, y, z], dtype=np.float32)
    n = np.linalg.norm(q) + 1e-12
    return q / n


# ---------------- geometry helpers ----------------
def backproject_depth_to_world(
    depth_png_path, T_c2w,
    fx, fy, cx, cy,
    depth_scale=1000.0,
    max_depth=4.0,
    sample_px=8,
    max_points=200000,
):
    """Backproject depth to world points (sparse sampling)."""
    depth = imageio.imread(depth_png_path)
    if depth.ndim != 2:
        raise ValueError(f"depth not HxW: {depth_png_path}")

    depth_m = depth.astype(np.float32) / float(depth_scale)
    H, W = depth_m.shape

    us = np.arange(0, W, sample_px, dtype=np.int32)
    vs = np.arange(0, H, sample_px, dtype=np.int32)
    uu, vv = np.meshgrid(us, vs)
    z = depth_m[vv, uu]

    valid = (z > 0.1) & (z < max_depth)
    if valid.sum() == 0:
        return np.zeros((0, 3), dtype=np.float32)

    uu = uu[valid].astype(np.float32)
    vv = vv[valid].astype(np.float32)
    z  = z[valid].astype(np.float32)

    x = (uu - cx) / fx * z
    y = (vv - cy) / fy * z
    pts_cam = np.stack([x, y, z, np.ones_like(z)], axis=1)  # (N,4)

    if pts_cam.shape[0] > max_points:
        idx = np.random.choice(pts_cam.shape[0], size=max_points, replace=False)
        pts_cam = pts_cam[idx]

    pts_w = (T_c2w @ pts_cam.T).T[:, :3].astype(np.float32)
    return pts_w


def voxelize_xyz(points_xyz, voxel):
    """(N,3) -> set(int64 packed voxel ids)"""
    if points_xyz is None or points_xyz.shape[0] == 0:
        return set()
    v = np.floor(points_xyz / float(voxel)).astype(np.int64)
    off = 1_000_000
    base = 2_000_001
    ix = v[:, 0] + off
    iy = v[:, 1] + off
    iz = v[:, 2] + off
    ids = ix * (base * base) + iy * base + iz
    return set(np.unique(ids).tolist())


def voxelize_xy(points_xyz, res):
    """(N,3) -> BEV 2D set(int64) using x,y only"""
    if points_xyz is None or points_xyz.shape[0] == 0:
        return set()
    v = np.floor(points_xyz[:, :2] / float(res)).astype(np.int64)
    off = 1_000_000
    base = 2_000_001
    ix = v[:, 0] + off
    iy = v[:, 1] + off
    ids = ix * base + iy
    return set(np.unique(ids).tolist())


def voxel_recall(V_rec: set, V_gt: set) -> float:
    """|V_rec âˆ© V_gt| / |V_gt|"""
    if len(V_gt) == 0:
        return 0.0
    inter = 0
    if len(V_rec) < len(V_gt):
        for x in V_rec:
            if x in V_gt:
                inter += 1
    else:
        for x in V_gt:
            if x in V_rec:
                inter += 1
    return float(inter) / float(len(V_gt))


# ---------------- KeySG-like room segmentation (III-A) ----------------
def estimate_floor_z(points_xyz):
    z = points_xyz[:, 2]
    z_low = np.quantile(z, 0.30)
    z_sel = z[z <= z_low]
    hist, edges = np.histogram(z_sel, bins=200)
    peak = int(np.argmax(hist))
    floor_z = 0.5 * (edges[peak] + edges[peak + 1])
    return float(floor_z)


def segment_rooms_from_cloud(points_xyz, bev_res=0.10, floor_band_abs=(0.02, 0.27), smooth_sigma=1.0):
    """
    Engineering room segmentation:
    - take points in absolute z floor band [z0,z1]
    - BEV occupancy -> gaussian -> watershed => regions
    - region -> axis-aligned bbox polygon (we store bbox only)
    """
    z0, z1 = floor_band_abs
    floor_pts = points_xyz[(points_xyz[:, 2] >= z0) & (points_xyz[:, 2] <= z1)]
    if floor_pts.shape[0] < 500:
        xy = points_xyz[:, :2]
        minxy = xy.min(axis=0); maxxy = xy.max(axis=0)
        return [{
            "bbox": (float(minxy[0]), float(minxy[1]), float(maxxy[0]), float(maxxy[1])),
            "z_range": (float(points_xyz[:, 2].min()), float(points_xyz[:, 2].max()))
        }]

    xy = floor_pts[:, :2]
    minxy = xy.min(axis=0); maxxy = xy.max(axis=0)
    W = int(np.ceil((maxxy[0] - minxy[0]) / bev_res)) + 1
    H = int(np.ceil((maxxy[1] - minxy[1]) / bev_res)) + 1

    ix = np.clip(((xy[:, 0] - minxy[0]) / bev_res).astype(np.int32), 0, W - 1)
    iy = np.clip(((xy[:, 1] - minxy[1]) / bev_res).astype(np.int32), 0, H - 1)

    occ = np.zeros((H, W), dtype=np.float32)
    np.add.at(occ, (iy, ix), 1.0)

    occ_s = gaussian(occ, sigma=smooth_sigma)
    mask = occ_s > 0.5

    dist = ndi.distance_transform_edt(mask)
    coords = peak_local_max(dist, footprint=np.ones((15, 15)), labels=mask)
    markers = np.zeros_like(dist, dtype=np.int32)
    for k, (r, c) in enumerate(coords, start=1):
        markers[r, c] = k

    if markers.max() == 0:
        return [{
            "bbox": (float(minxy[0]), float(minxy[1]), float(maxxy[0]), float(maxxy[1])),
            "z_range": (float(points_xyz[:, 2].min()), float(points_xyz[:, 2].max()))
        }]

    labels = watershed(-dist, markers, mask=mask)

    rooms = []
    for lab in range(1, labels.max() + 1):
        region = (labels == lab)
        if region.sum() < 200:
            continue
        ys, xs = np.where(region)
        minx = xs.min() * bev_res + minxy[0]
        maxx = (xs.max() + 1) * bev_res + minxy[0]
        miny = ys.min() * bev_res + minxy[1]
        maxy = (ys.max() + 1) * bev_res + minxy[1]
        bbox = (float(minx), float(miny), float(maxx), float(maxy))

        x0, y0, x1, y1 = bbox
        dense_xy = points_xyz[:, :2]
        in_bbox = (dense_xy[:, 0] >= x0) & (dense_xy[:, 0] <= x1) & (dense_xy[:, 1] >= y0) & (dense_xy[:, 1] <= y1)
        if in_bbox.sum() < 200:
            zrg = (float(points_xyz[:, 2].min()), float(points_xyz[:, 2].max()))
        else:
            z_inside = points_xyz[in_bbox, 2]
            zrg = (float(np.quantile(z_inside, 0.01)), float(np.quantile(z_inside, 0.99)))

        rooms.append({"bbox": bbox, "z_range": zrg})

    if len(rooms) == 0:
        xy_all = points_xyz[:, :2]
        minxy = xy_all.min(axis=0); maxxy = xy_all.max(axis=0)
        rooms = [{
            "bbox": (float(minxy[0]), float(minxy[1]), float(maxxy[0]), float(maxxy[1])),
            "z_range": (float(points_xyz[:, 2].min()), float(points_xyz[:, 2].max()))
        }]
    return rooms


# ---------------- room assign & filter (bbox fast) ----------------
def frac_in_bbox_xy(points_w, bbox):
    if points_w.shape[0] == 0:
        return 0.0
    x0, y0, x1, y1 = bbox
    xy = points_w[:, :2]
    inside = (xy[:, 0] >= x0) & (xy[:, 0] <= x1) & (xy[:, 1] >= y0) & (xy[:, 1] <= y1)
    return float(inside.mean())


def assign_room(camera_center, pts_w, rooms, eta_assign=0.12, z_margin=0.10):
    """
    1) try camera center inside bbox & z range
    2) fallback: choose room with max frac(pts in bbox) if >= eta_assign
    """
    x, y, z = float(camera_center[0]), float(camera_center[1]), float(camera_center[2])

    # pass1: camera center
    for rid, r in enumerate(rooms):
        (z0, z1) = r["z_range"]
        if (z >= z0 - z_margin) and (z <= z1 + z_margin):
            x0, y0, x1, y1 = r["bbox"]
            if (x >= x0) and (x <= x1) and (y >= y0) and (y <= y1):
                return rid

    # pass2: by points overlap
    best_rid, best_frac = None, 0.0
    for rid, r in enumerate(rooms):
        frac = frac_in_bbox_xy(pts_w, r["bbox"])
        if frac > best_frac:
            best_frac = frac
            best_rid = rid

    if best_rid is not None and best_frac >= float(eta_assign):
        return best_rid
    return None


def keep_frame_in_room(pts_w, room_bbox, eta=0.30):
    frac = frac_in_bbox_xy(pts_w, room_bbox)
    return (frac >= float(eta)), frac


# ---------------- clustering ----------------
def cluster_poses_dbscan(pose_feats_7d, eps=0.25, min_samples=2):
    scaler = StandardScaler()
    X = scaler.fit_transform(pose_feats_7d)
    labels = DBSCAN(eps=float(eps), min_samples=int(min_samples)).fit_predict(X)
    return labels, X


def medoid_indices(X_std, indices_in_room, labels):
    keyframes = []
    labs = sorted(set(labels.tolist()))
    for lab in labs:
        if lab == -1:
            continue
        idxs = np.where(labels == lab)[0]
        if idxs.size == 0:
            continue
        C = X_std[idxs]
        d = np.linalg.norm(C[:, None, :] - C[None, :, :], axis=2)
        med_local = int(idxs[np.argmin(d.sum(axis=1))])
        keyframes.append(int(indices_in_room[med_local]))
    return keyframes


# ---------------- GT build (tsdf_full or raw_union) ----------------
def tsdf_full_reconstruct(
    depth_paths, pose_paths,
    fx, fy, cx, cy,
    depth_scale=1000.0,
    max_depth=4.0,
    pose_is_w2c=False,
    tsdf_voxel=0.02,
    sdf_trunc=0.04,
    stride=1,
):
    d0 = imageio.imread(depth_paths[0])
    H, W = d0.shape[:2]
    intrinsic = o3d.camera.PinholeCameraIntrinsic(int(W), int(H), float(fx), float(fy), float(cx), float(cy))

    volume = o3d.pipelines.integration.ScalableTSDFVolume(
        voxel_length=float(tsdf_voxel),
        sdf_trunc=float(sdf_trunc),
        color_type=o3d.pipelines.integration.TSDFVolumeColorType.NoColor
    )

    color_dummy = np.zeros((H, W, 3), dtype=np.uint8)
    color_o3d = o3d.geometry.Image(color_dummy)

    idxs = list(range(0, len(depth_paths), max(1, int(stride))))
    for idx in tqdm(idxs, desc="Build TSDF GT", leave=False):
        depth_raw = imageio.imread(depth_paths[idx]).astype(np.uint16)
        depth_o3d = o3d.geometry.Image(depth_raw)
        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(
            color_o3d, depth_o3d,
            depth_scale=float(depth_scale),
            depth_trunc=float(max_depth),
            convert_rgb_to_intensity=False
        )

        T_file = np.loadtxt(pose_paths[idx]).astype(np.float32)
        # Open3D TSDF expects extrinsic = world_to_camera (w2c)
        if pose_is_w2c:
            T_w2c = T_file
        else:
            T_w2c = np.linalg.inv(T_file)

        volume.integrate(rgbd, intrinsic, T_w2c)

    return volume.extract_point_cloud()


def build_gt_voxels_raw_union(
    depth_paths, pose_paths,
    fx, fy, cx, cy,
    depth_scale, max_depth, sample_px,
    pose_is_w2c,
    metric,
    voxel_cov, bev_res,
    floor_band_abs,
    stride=1,
):
    V_gt = set()
    idxs = list(range(0, len(depth_paths), max(1, int(stride))))
    for idx in tqdm(idxs, desc="Build RAW GT voxels", leave=False):
        T = load_pose_4x4(pose_paths[idx], pose_is_w2c=pose_is_w2c)
        pts = backproject_depth_to_world(
            depth_paths[idx], T, fx, fy, cx, cy,
            depth_scale=depth_scale, max_depth=max_depth, sample_px=sample_px
        )
        if metric == "bev":
            z0, z1 = floor_band_abs
            pts = pts[(pts[:, 2] >= z0) & (pts[:, 2] <= z1)]
            V_gt |= voxelize_xy(pts, bev_res)
        else:  # raw3d
            V_gt |= voxelize_xyz(pts, voxel_cov)
    return V_gt


# ---------------- per-frame voxel set (IMPORTANT: use backprojection, NOT single-frame TSDF) ----------------
def frame_voxels_from_backproj(
    frame_idx, depth_paths, pose_paths,
    fx, fy, cx, cy,
    depth_scale, max_depth, sample_px,
    pose_is_w2c,
    metric,
    voxel_cov, bev_res,
    floor_band_abs,
    room_bbox=None,
):
    T = load_pose_4x4(pose_paths[frame_idx], pose_is_w2c=pose_is_w2c)
    pts = backproject_depth_to_world(
        depth_paths[frame_idx], T, fx, fy, cx, cy,
        depth_scale=depth_scale, max_depth=max_depth, sample_px=sample_px
    )

    if room_bbox is not None and pts.shape[0] > 0:
        x0, y0, x1, y1 = room_bbox
        xy = pts[:, :2]
        mask = (xy[:, 0] >= x0) & (xy[:, 0] <= x1) & (xy[:, 1] >= y0) & (xy[:, 1] <= y1)
        pts = pts[mask]

    if metric == "bev":
        z0, z1 = floor_band_abs
        pts = pts[(pts[:, 2] >= z0) & (pts[:, 2] <= z1)]
        return voxelize_xy(pts, bev_res)
    else:
        return voxelize_xyz(pts, voxel_cov)


def pick_topk_by_voxelsize(
    frame_indices, depth_paths, pose_paths,
    fx, fy, cx, cy,
    depth_scale, max_depth, sample_px,
    pose_is_w2c,
    metric,
    voxel_cov, bev_res,
    floor_band_abs,
    room_bbox,
    topk,
):
    scored = []
    for f in frame_indices:
        V = frame_voxels_from_backproj(
            f, depth_paths, pose_paths,
            fx, fy, cx, cy,
            depth_scale, max_depth, sample_px,
            pose_is_w2c,
            metric, voxel_cov, bev_res,
            floor_band_abs,
            room_bbox=room_bbox
        )
        scored.append((int(f), len(V)))
    scored.sort(key=lambda x: x[1], reverse=True)
    return [f for f, _ in scored[:topk]]


# ---------------- greedy selection ----------------
def greedy_select(candidate_frames, V_frame_map, V_gt, target_cov=0.85, max_K=80, min_gain_ratio=0.001, verbose=True):
    selected = []
    V_sel = set()
    cov = 0.0
    V_gt_size = max(1, len(V_gt))

    remaining = set(candidate_frames)

    while len(remaining) > 0 and len(selected) < int(max_K) and cov < float(target_cov):
        best_f, best_gain = None, -1
        for f in remaining:
            V = V_frame_map[f]
            gain = len(V - V_sel)
            if gain > best_gain:
                best_gain = gain
                best_f = f

        if best_f is None or best_gain <= 0:
            break

        gain_ratio = best_gain / float(V_gt_size)
        if gain_ratio < float(min_gain_ratio):
            break

        selected.append(int(best_f))
        V_sel |= V_frame_map[best_f]
        cov = len(V_sel & V_gt) / float(V_gt_size)

        remaining.remove(best_f)

        if verbose:
            print(f"[KeySG] greedy add frame={best_f} gain={best_gain} gain_ratio={gain_ratio:.6f} cov={cov*100:.2f}%")

    return selected, cov, V_sel


def main():
    ap = argparse.ArgumentParser("KeySG-style keyframe sampling + (paper-aligned) coverage. Backproj voxels to avoid TSDF mismatch.")

    ap.add_argument("--scene_dir", required=True)
    ap.add_argument("--scene_id", default="")
    ap.add_argument("--out", required=True)

    # intrinsics
    ap.add_argument("--use_intrinsic", choices=["auto", "depth", "color"], default="auto")
    ap.add_argument("--pose_is_w2c", action="store_true")

    # depth -> points
    ap.add_argument("--depth_scale", type=float, default=1000.0)
    ap.add_argument("--max_depth", type=float, default=4.0)
    ap.add_argument("--sample_px", type=int, default=4)            # default adopt friend's idea: denser
    ap.add_argument("--candidate_stride", type=int, default=1)

    # metric / coverage
    ap.add_argument("--metric", choices=["bev", "raw3d"], default="bev",
                    help="bev: floor-band BEV (paper-aligned); raw3d: 3D voxel recall using backproj points")
    ap.add_argument("--voxel_cov", type=float, default=0.10, help="3D voxel size for raw3d")
    ap.add_argument("--bev_res", type=float, default=0.10, help="BEV grid resolution (also used for room segmentation)")
    ap.add_argument("--floor_low", type=float, default=0.02)
    ap.add_argument("--floor_high", type=float, default=0.25)

    # GT source
    ap.add_argument("--gt_source", choices=["tsdf_full", "raw_union"], default="tsdf_full",
                    help="tsdf_full: build fused TSDF GT; raw_union: union of backproject voxels (removes TSDF mismatch)")
    ap.add_argument("--gt_stride", type=int, default=1, help="stride when building raw_union GT (speed)")
    ap.add_argument("--gt_tsdf_voxel", type=float, default=0.02)
    ap.add_argument("--gt_sdf_trunc", type=float, default=0.04)

    # room & filtering
    ap.add_argument("--eta", type=float, default=0.30, help="keep frame if frac(pts in room) >= eta")
    ap.add_argument("--eta_assign", type=float, default=0.12, help="fallback assignment if best room frac >= eta_assign")

    # DBSCAN (defaults adopt friend's direction: more clusters)
    ap.add_argument("--db_eps", type=float, default=0.25)
    ap.add_argument("--db_min_samples", type=int, default=2)
    ap.add_argument("--rot_w", type=float, default=3.0)

    # noise / fallback fixes
    ap.add_argument("--noise_topk", type=int, default=8, help="add top-k noise frames per room back into candidates")
    ap.add_argument("--fallback_topk", type=int, default=10, help="if a room has 0 clusters, pick top-k by voxel size")

    # greedy selection
    ap.add_argument("--target_cov", type=float, default=0.85)
    ap.add_argument("--min_gain_ratio", type=float, default=0.001)
    ap.add_argument("--max_K", type=int, default=80)

    ap.add_argument("--seed", type=int, default=2024)

    args = ap.parse_args()
    np.random.seed(args.seed)

    scene_dir = args.scene_dir
    scene_id = args.scene_id if args.scene_id else os.path.basename(scene_dir.rstrip("/"))

    # files
    color_paths, depth_paths, pose_paths = list_scannet_files(scene_dir)
    N = len(depth_paths)
    cand_all = list(range(0, N, max(1, int(args.candidate_stride))))
    print(f"[KeySG] scene={scene_id} frames_total={N} candidates={len(cand_all)}")

    # choose intrinsics matching depth resolution
    depth0 = imageio.imread(depth_paths[0])
    H, W = depth0.shape[:2]
    intrinsic_dir = os.path.join(scene_dir, "intrinsic")
    intr_color = os.path.join(intrinsic_dir, "intrinsic_color.txt")
    intr_depth = os.path.join(intrinsic_dir, "intrinsic_depth.txt")

    if args.use_intrinsic == "depth":
        chosen = intr_depth
    elif args.use_intrinsic == "color":
        chosen = intr_color
    else:
        best = None
        for name, path in [("color", intr_color), ("depth", intr_depth)]:
            if not os.path.isfile(path):
                continue
            fx_, fy_, cx_, cy_, _ = load_intrinsics_txt_4x4(path)
            score = abs(cx_ - (W / 2.0)) + abs(cy_ - (H / 2.0))
            if best is None or score < best[0]:
                best = (score, name, path)
        if best is None:
            raise FileNotFoundError(f"No intrinsic txt found in {intrinsic_dir}")
        chosen = best[2]

    fx, fy, cx, cy, _ = load_intrinsics_txt_4x4(chosen)
    print(f"[KeySG] depth resolution HxW={H}x{W}")
    print(f"[KeySG] intrinsics: {chosen}")
    print(f"[KeySG] fx={fx:.2f} fy={fy:.2f} cx={cx:.2f} cy={cy:.2f}")

    # ---- build GT pointcloud/voxels ----
    floor_band_abs = None
    # floor band is absolute based on estimated floor z from GT pointcloud
    if args.gt_source == "tsdf_full":
        print(f"[KeySG] GT source = tsdf_full. Building TSDF from frames={N} stride=1")
        gt_pcd = tsdf_full_reconstruct(
            depth_paths, pose_paths,
            fx, fy, cx, cy,
            depth_scale=args.depth_scale,
            max_depth=args.max_depth,
            pose_is_w2c=args.pose_is_w2c,
            tsdf_voxel=args.gt_tsdf_voxel,
            sdf_trunc=args.gt_sdf_trunc,
            stride=1
        )
        gt_pts = np.asarray(gt_pcd.points).astype(np.float32)
        print(f"[KeySG] tsdf_full points={gt_pts.shape[0]}")
        floor_z = estimate_floor_z(gt_pts)
        floor_band_abs = (floor_z - float(args.floor_low), floor_z + float(args.floor_high))
    else:
        # raw_union still needs a floor estimate for bev rooms; get a cheap floor estimate from a sparse sample of frames
        sample_for_floor = list(range(0, N, max(1, N // 50)))
        pts_acc = []
        for idx in sample_for_floor:
            T = load_pose_4x4(pose_paths[idx], pose_is_w2c=args.pose_is_w2c)
            pts = backproject_depth_to_world(
                depth_paths[idx], T, fx, fy, cx, cy,
                depth_scale=args.depth_scale, max_depth=args.max_depth, sample_px=max(8, args.sample_px)
            )
            if pts.shape[0] > 0:
                pts_acc.append(pts)
        if len(pts_acc) == 0:
            raise RuntimeError("Cannot estimate floor: no backproject points.")
        gt_pts_floor = np.concatenate(pts_acc, axis=0)
        floor_z = estimate_floor_z(gt_pts_floor)
        floor_band_abs = (floor_z - float(args.floor_low), floor_z + float(args.floor_high))

    # build rooms from GT geometry
    if args.gt_source == "tsdf_full":
        rooms = segment_rooms_from_cloud(gt_pts, bev_res=args.bev_res, floor_band_abs=floor_band_abs, smooth_sigma=1.0)
    else:
        # for raw_union, build a pseudo cloud for room segmentation using the floor estimate sample
        rooms = segment_rooms_from_cloud(gt_pts_floor, bev_res=args.bev_res, floor_band_abs=floor_band_abs, smooth_sigma=1.0)

    print(f"[KeySG] rooms segmented: {len(rooms)} (bev_res={args.bev_res})")

    # Build V_gt
    if args.gt_source == "tsdf_full":
        if args.metric == "bev":
            z0, z1 = floor_band_abs
            gt_floor = gt_pts[(gt_pts[:, 2] >= z0) & (gt_pts[:, 2] <= z1)]
            V_gt = voxelize_xy(gt_floor, args.bev_res)
            print(f"[KeySG] V_gt_size = {len(V_gt)} at bev_res={args.bev_res}m (gt_source=tsdf_full, floor_band=[{z0:.3f},{z1:.3f}])")
        else:
            V_gt = voxelize_xyz(gt_pts, args.voxel_cov)
            print(f"[KeySG] V_gt_size = {len(V_gt)} at voxel={args.voxel_cov}m (gt_source=tsdf_full)")
    else:
        print(f"[KeySG] GT source = raw_union. Building backproj GT voxels stride={args.gt_stride} sample_px={args.sample_px}")
        V_gt = build_gt_voxels_raw_union(
            depth_paths, pose_paths,
            fx, fy, cx, cy,
            depth_scale=args.depth_scale, max_depth=args.max_depth, sample_px=args.sample_px,
            pose_is_w2c=args.pose_is_w2c,
            metric=args.metric,
            voxel_cov=args.voxel_cov, bev_res=args.bev_res,
            floor_band_abs=floor_band_abs,
            stride=args.gt_stride
        )
        if args.metric == "bev":
            print(f"[KeySG] V_gt_size = {len(V_gt)} at bev_res={args.bev_res}m (gt_source=raw_union)")
        else:
            print(f"[KeySG] V_gt_size = {len(V_gt)} at voxel={args.voxel_cov}m (gt_source=raw_union)")

    # ---- Step A: assign + filter frames into rooms ----
    room_frames = {rid: [] for rid in range(len(rooms))}
    room_posefeats = {rid: [] for rid in range(len(rooms))}
    kept_total = 0

    for idx in tqdm(cand_all, desc="Assign+filter frames"):
        T = load_pose_4x4(pose_paths[idx], pose_is_w2c=args.pose_is_w2c)
        cam_center = T[:3, 3]

        pts_w = backproject_depth_to_world(
            depth_paths[idx], T,
            fx, fy, cx, cy,
            depth_scale=args.depth_scale,
            max_depth=args.max_depth,
            sample_px=args.sample_px
        )

        rid = assign_room(cam_center, pts_w, rooms, eta_assign=args.eta_assign)
        if rid is None:
            continue

        keep, _frac = keep_frame_in_room(pts_w, rooms[rid]["bbox"], eta=args.eta)
        if not keep:
            continue

        R = T[:3, :3]
        t = cam_center.astype(np.float32)
        q = rot_to_quat_wxyz(R).astype(np.float32)
        feat = np.concatenate([t, args.rot_w * q], axis=0)

        room_frames[rid].append(int(idx))
        room_posefeats[rid].append(feat)
        kept_total += 1

    print(f"[KeySG] frames kept after room+eta filter = {kept_total}")

    # ---- Step B: DBSCAN + medoid + (IMPORTANT) keep noise topK + fallback for empty-cluster rooms ----
    candidate = []
    per_room = {}

    for rid in range(len(rooms)):
        idxs = room_frames[rid]
        if len(idxs) == 0:
            per_room[rid] = {"kept": 0, "clusters": 0, "noise": 0, "keyframes": 0}
            continue

        F = np.stack(room_posefeats[rid], axis=0)
        labels, X_std = cluster_poses_dbscan(F, eps=args.db_eps, min_samples=args.db_min_samples)

        labs = set(labels.tolist())
        num_clusters = len(labs) - (1 if -1 in labs else 0)

        # medoids for real clusters
        kf_r = medoid_indices(X_std, idxs, labels)

        # noise indices
        noise_local = np.where(labels == -1)[0]
        noise_idxs = [int(idxs[i]) for i in noise_local.tolist()]

        # fallback: if no clusters (your room2 case), do NOT lose this room
        if num_clusters == 0:
            kf_r = pick_topk_by_voxelsize(
                idxs, depth_paths, pose_paths,
                fx, fy, cx, cy,
                args.depth_scale, args.max_depth, args.sample_px,
                args.pose_is_w2c,
                metric=args.metric,
                voxel_cov=args.voxel_cov,
                bev_res=args.bev_res,
                floor_band_abs=floor_band_abs,
                room_bbox=rooms[rid]["bbox"],
                topk=min(args.fallback_topk, len(idxs))
            )
            noise_idxs = []

        # always add some noise back (rare views)
        if len(noise_idxs) > 0 and args.noise_topk > 0:
            noise_pick = pick_topk_by_voxelsize(
                noise_idxs, depth_paths, pose_paths,
                fx, fy, cx, cy,
                args.depth_scale, args.max_depth, args.sample_px,
                args.pose_is_w2c,
                metric=args.metric,
                voxel_cov=args.voxel_cov,
                bev_res=args.bev_res,
                floor_band_abs=floor_band_abs,
                room_bbox=rooms[rid]["bbox"],
                topk=min(args.noise_topk, len(noise_idxs))
            )
            kf_r = list(set(kf_r + noise_pick))

        candidate.extend(kf_r)

        per_room[rid] = {
            "kept": int(len(idxs)),
            "clusters": int(num_clusters),
            "noise": int(len(noise_local)),
            "keyframes": int(len(kf_r)),
        }

    candidate = sorted(list(set(candidate)))
    print(f"[KeySG] candidate keyframes (room+dbscan+medoid+noise/fallback) = {len(candidate)}")
    print(f"[KeySG] per-room stats = {per_room}")
    print(f"[KeySG] candidate_indices (first 30) = {candidate[:30]}")

    # ---- Candidate voxel sets (CRITICAL FIX: use backprojection, not single-frame TSDF) ----
    V_frame_map = {}
    print("[KeySG] Candidate voxel sets (from backprojection):")
    for f in tqdm(candidate, desc="Candidate voxel sets"):
        V_one = frame_voxels_from_backproj(
            f, depth_paths, pose_paths,
            fx, fy, cx, cy,
            args.depth_scale, args.max_depth, args.sample_px,
            args.pose_is_w2c,
            metric=args.metric,
            voxel_cov=args.voxel_cov, bev_res=args.bev_res,
            floor_band_abs=floor_band_abs,
            room_bbox=None
        )
        V_frame_map[int(f)] = V_one

    # ---- Greedy ----
    selected, achieved_cov, V_sel = greedy_select(
        candidate_frames=candidate,
        V_frame_map=V_frame_map,
        V_gt=V_gt,
        target_cov=args.target_cov,
        max_K=args.max_K,
        min_gain_ratio=args.min_gain_ratio,
        verbose=True
    )

    print(f"[KeySG] FINAL selected keyframes = {len(selected)} achieved_cov={achieved_cov*100:.2f}% (metric={args.metric}, gt_source={args.gt_source})")

    out = {
        "scene_id": scene_id,
        "method": "keysg_room_dbscan_medoid_greedy_backproj_voxels",
        "frames_total": int(N),
        "candidate_stride": int(args.candidate_stride),
        "intrinsic_path": chosen,
        "depth_shape_hw": [int(H), int(W)],

        "depth_scale": float(args.depth_scale),
        "max_depth": float(args.max_depth),
        "sample_px": int(args.sample_px),

        "metric": args.metric,
        "gt_source": args.gt_source,
        "gt": {
            "gt_stride": int(args.gt_stride),
            "gt_tsdf_voxel": float(args.gt_tsdf_voxel),
            "gt_sdf_trunc": float(args.gt_sdf_trunc),
        },
        "room_seg": {
            "bev_res": float(args.bev_res),
            "floor_band_abs": [float(floor_band_abs[0]), float(floor_band_abs[1])],
            "num_rooms": int(len(rooms)),
            "eta": float(args.eta),
            "eta_assign": float(args.eta_assign),
        },
        "dbscan": {
            "rot_w": float(args.rot_w),
            "eps": float(args.db_eps),
            "min_samples": int(args.db_min_samples),
            "noise_topk": int(args.noise_topk),
            "fallback_topk": int(args.fallback_topk),
        },
        "greedy": {
            "target_cov": float(args.target_cov),
            "min_gain_ratio": float(args.min_gain_ratio),
            "max_K": int(args.max_K),
        },
        "coverage": {
            "V_gt_size": int(len(V_gt)),
            "V_sel_size": int(len(V_sel)),
            "achieved_cov": float(achieved_cov),
            "voxel_cov": float(args.voxel_cov),
            "bev_res": float(args.bev_res),
        },
        "candidate_indices": candidate,
        "keyframe_indices": selected,
    }

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    with open(args.out, "w") as f:
        json.dump(out, f, indent=2)

    print(f"[KeySG] Saved: {args.out}")


if __name__ == "__main__":
    main()
